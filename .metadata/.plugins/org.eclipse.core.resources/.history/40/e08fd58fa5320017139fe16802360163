package MT;

import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CollectionUtils;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.StringUtils;

public class CoreNLP 
{
	private Properties props;
	private StanfordCoreNLP pipeline;
	private TrainController tc;


	//Have to remove Parameters if want to do it full scale
	//	public CoreNLP(String textEng, String textEsp)
	public CoreNLP()
	{
		props = new Properties();
		props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner");
		pipeline = new StanfordCoreNLP(props);
		tc = new TrainController();

		// Se puede hacer siempre la creacion del StanfordCoreNLP pero creo que eso solo
		// se quiere una vez

		//Toca crear una Anotation por documento? 
		//O una anotation por "ROW"
		//Si esto toca hacer toca agregar otra clase que maneje esto

		//process(textEng,textEsp);
	}


	//Podria ir en otra clase
	public void process(String textEng, String textEsp)
	{
		System.out.println("RAW TEXT:"+ textEng);
		System.out.println("RAW TEXT:"+ textEsp);
		//Spanish Annotation --> Doesn't offer right now CORENLP
		Annotation documentEng = new Annotation(textEng);
		Annotation documentEsp = new Annotation(textEsp);

		//System.out.println("Annotation TEXT:"+ documentEng);
		//System.out.println("Annotation TEXT:"+ documentEsp);
		pipeline.annotate(documentEng);
		pipeline.annotate(documentEsp);

		List<CoreMap>  sentEng = documentEng.get(SentencesAnnotation.class);
		List<CoreMap>  sentEsp = documentEsp.get(SentencesAnnotation.class);

		System.out.println("COREMAP TEXT:"+ sentEng);
		System.out.println("COREMAP TEXT:"+ sentEsp);
		//Different size of sentences
		int mayor = Math.max(sentEng.size(), sentEsp.size());

		//DID THIS OUT OF DESPERATION 
		//TOo many bugs 
		int min = Math.min(sentEng.size(), sentEsp.size());


		int i;
		System.out.println("Size of eNGLI: "+sentEng.size());
		System.out.println("Size of espLI: "+sentEsp.size());
		System.out.println("MAYOR: "+mayor);

		for(i=0;i<min;i++)
		{
			//Potencial de investigacion
			//Invocar qué método es más fácil
			//Si se implementa una función auxiliar que
			//retorne unos valores
			//O realizar un if statement para saber cual get. (ya que son dif size)
			if(sentEng.size()==0 && sentEsp.size()>0)
			{
				sentEsp.get(i);
			}
			//Check the IFs manually
			else if (sentEsp.size()==0 && sentEng.size()>0)
			{
				sentEng.get(i);
			}
			else
			{

				//PRINTS SENTENCES TO STRINGS
				//	if(sentEsp.size()>) why dis no?


				CoreMap esp = 	sentEsp.get(i);
				CoreMap eng = 	sentEng.get(i);

				//Check other data type instead of ArrayList
				//Gets en array with sentences tokenized
				//ArrayList spanishSent = (ArrayList) esp.get(CoreAnnotations.TokensAnnotation.class);
				//ArrayList englishSent = (ArrayList) eng.get(CoreAnnotations.TokensAnnotation.class);

				List<CoreLabel> spanishSent =  esp.get(CoreAnnotations.TokensAnnotation.class);
				List<CoreLabel> englishSent =  eng.get(CoreAnnotations.TokensAnnotation.class);

				//Searching for bugFix that excludes last point.
				//System.out.println("TestModulos: "+englishSent.size()%3);
				//System.out.println("TestModulos: "+spanishSent.size()%3);
				//this is one approach to fix the bug pero hay otro que es un 
				//workaround

				//System.out.println("TokenSizeEng: "+englishSent.size());
				//System.out.println("TokenSizeEsp: "+spanishSent.size());

				//System.out.println("TokensEng: "+ englishSent.get(0));
				//System.out.println("TokensSpa: "+spanishSent);

				//Gets n-grams with min 3 and max 3
				//SEE IF YOU CAN MIX THIS METHOD WITH YOUR IMPLEMENTATION OF GRAMS
				//englishSent = CollectionUtils.getNGrams((List) englishSent, 3,3);
				//spanishSent = CollectionUtils.getNGrams((List) spanishSent, 3, 3);

				/*Pseudocode
				i=14
				while()
				i+1(diferencia) ==0 && i
					resolve:  se incluye la interseccion de todos y se añade the last.
				 */


				//Design idea have a pipeline where each sentence is processed
				//Where each sentence would have an n-gram model and count probability

				//Have n-gram output is with parentesis
				//Can take them out with regex (regular expressions)?

				//TOKENSIZE
				System.out.println("SizeEng: "+englishSent.size());
				System.out.println("SizeEsp: "+spanishSent.size());



				//loopNGrams(englishSent,spanishSent);
				System.out.println("TokensEng: "+ englishSent);
				System.out.println("TokensSpa: "+spanishSent);


				addNgrams(englishSent, spanishSent);


			}
		}

		tc.getProbs();

	}

	public static void main (String[] args)
	{
		//String testEng = "Unity uses <span class=\"doc-keyword\">Animation Layers</span> for managing complex state machines for different body parts. An example of this is if you have a lower-body layer for walking-jumping, and an upper-body layer for throwing objects / shooting.";
		//String testEsp = "Unity usa <span class=\"doc-keyword\">Animation Layers</span> para manejar state machines complejos para differente partes del cuerpo. Un ejemplo de esto es si se tiene una capa inferior de cuerpo para caminar-saltar, y una capa superior de cuerpo para tirar objetos / disparar.";

		//new CoreNLP(testEng,testEsp);
	}

	private void loopNGrams(List englishSent, List spanishSent)
	{
		int min = Math.min(englishSent.size(),spanishSent.size());
		int i;
		//Have to think of somethin (look above) to fix the different sizes
		for(i = 0; i<min;i++)
		{
			//It is object,but object of what type
			//Workaround quick make it tostring
			//PairNGram pair = new PairNGram(englishSent.get(i).toString(),spanishSent.get(i).toString());
		}

	}

	private void addNgrams(List<CoreLabel> englishSent , List<CoreLabel> spanishSent)
	{
		Grams gr =  new Grams(3);
		gr.operate(englishSent, spanishSent);
		tc.addPair(gr.getEngGrams(),	gr.getEspGrams());
	}

	private void addNgramsTranslate(List<CoreLabel> englishSent )
	{
		Grams gr =  new Grams(3,true);
		gr.translate(englishSent);
	}

	public TrainController giveTC()
	{
		return tc;
	}

	public StanfordCoreNLP getNLP()
	{
		return pipeline;
	}

	public void translate(String textEng, String textEsp)
	{
		System.out.println("RAW TEXT:"+ textEng);
		Annotation documentEng = new Annotation(textEng);

		pipeline.annotate(documentEng);

		List<CoreMap>  sentEng = documentEng.get(SentencesAnnotation.class);

		System.out.println("COREMAP TEXT:"+ sentEng);
		//Different size of sentences


		int i;
		int min = sentEng.size();
		for(i=0;i<min;i++)
		{

			//PRINTS SENTENCES TO STRINGS
			//	if(sentEsp.size()>) why dis no?
			CoreMap eng = 	sentEng.get(i);
			List<CoreLabel> englishSent =  eng.get(CoreAnnotations.TokensAnnotation.class);

			//TOKENSIZE
			System.out.println("SizeEng: "+englishSent.size());
			//loopNGrams(englishSent,spanishSent);
			System.out.println("TokensEng: "+ englishSent);

		}

	}


}
